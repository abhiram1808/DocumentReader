// src/services/llmService.js
const { PDFLoader } = require('@langchain/community/document_loaders/fs/pdf');
const { RecursiveCharacterTextSplitter } = require('langchain/text_splitter');
const { GoogleGenerativeAIEmbeddings } = require('@langchain/google-genai');
const { ChatGoogleGenerativeAI } = require('@langchain/google-genai');
const { RetrievalQAChain } = require('langchain/chains');
const { PromptTemplate } = require('@langchain/core/prompts');
const documentModel = require('../models/documentModel'); // Import document model
const { Document } = require('@langchain/core/documents'); // Import Document class

// Initialize Google Gemini LLM (ensure GOOGLE_API_KEY is in .env)
const model = new ChatGoogleGenerativeAI({
  model: 'gemini-1.5-flash',
  apiKey: process.env.GOOGLE_API_KEY,
  temperature: 0.2,
  maxOutputTokens: 2048,
});

// Initialize Google Gemini Embeddings
const embeddings = new GoogleGenerativeAIEmbeddings({
  model: 'embedding-001',
  apiKey: process.env.GOOGLE_API_KEY,
});

/**
 * Processes a PDF document: loads, splits, embeds, and stores in vector DB for a specific document ID.
 * @param {string} documentId - The unique ID for this document.
 * @param {string} filePath - Path to the PDF file.
 */
const processPdfDocument = async (documentId, filePath) => { // <-- documentId added
  console.log('Loading PDF...');
  const loader = new PDFLoader(filePath);
  const docs = await loader.load();
  console.log(`Loaded ${docs.length} pages.`);

  if (docs.length === 0) {
    throw new Error('No content found in the PDF document.');
  }

  console.log('Splitting documents...');
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  const splitDocs = await textSplitter.splitDocuments(docs);
  console.log(`Split into ${splitDocs.length} chunks.`);

  console.log(`Creating/Updating vector store for document ID: ${documentId}...`);
  // Pass documentId to documentModel.createAndPersistVectorStore
  await documentModel.createAndPersistVectorStore(documentId, splitDocs, embeddings);
  console.log('Vector store created/updated successfully.');
};

/**
 * Loads a specific document's context (vector store and raw chunks) into backend memory.
 * @param {string} documentId - The unique ID of the document to load.
 */
const loadDocumentContext = async (documentId) => { // <-- NEW FUNCTION
  console.log(`Loading document context for ID: ${documentId}...`);
  await documentModel.loadSpecificVectorStore(documentId, embeddings);
  console.log(`Document context for ID: ${documentId} loaded successfully.`);
};

/**
 * Gets an answer to a question from the loaded document knowledge base.
 * @param {string} question - The question to ask.
 * @returns {Promise<string>} The answer generated by the LLM.
 */
const getAnswerFromDocument = async (question) => {
  console.log(`Getting answer for question: "${question}"`);
  const vectorStore = await documentModel.getVectorStore(embeddings); // This now gets the currently loaded store

  if (!vectorStore) {
    throw new Error('No document knowledge base loaded. Please upload or load a document first.');
  }

  const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());

  const response = await chain.call({ query: question });
  return response.text;
};

/**
 * Generates a summary of the entire loaded document.
 * @returns {Promise<string>} The summary generated by the LLM.
 */
const getDocumentSummary = async () => {
  console.log('Generating document summary...');
  const allChunks = documentModel.getAllDocumentChunks(); // Gets chunks of currently loaded document

  if (allChunks.length === 0) {
    throw new Error('No document chunks available for summarization. Please upload or load a document first.');
  }

  const fullDocumentText = allChunks.map(doc => doc.pageContent).join('\n\n');

  const summaryPrompt = PromptTemplate.fromTemplate(
    `Please provide a comprehensive summary of the following document. Focus on main ideas, key arguments, and important conclusions.
    
    Document:
    {document}
    
    Summary:`
  );

  const result = await model.invoke(await summaryPrompt.format({ document: fullDocumentText }));
  return result.content;
};

/**
 * Extracts key concepts from the loaded document.
 * @returns {Promise<string[]>} An array of key concepts.
 */
const getKeyConcepts = async () => {
  console.log('Extracting key concepts...');
  const allChunks = documentModel.getAllDocumentChunks(); // Gets chunks of currently loaded document

  if (allChunks.length === 0) {
    throw new Error('No document chunks available for key concept extraction. Please upload or load a document first.');
  }

  const fullDocumentText = allChunks.map(doc => doc.pageContent).join('\n\n');

  const conceptsPrompt = PromptTemplate.fromTemplate(
    `From the following document, identify and list the most important key concepts, terms, and topics. Provide them as a comma-separated list.
    
    Document:
    {document}
    
    Key Concepts (comma-separated):`
  );

  const result = await model.invoke(await conceptsPrompt.format({ document: fullDocumentText }));
  return result.content.split(',').map(concept => concept.trim()).filter(concept => concept.length > 0);
};

/**
 * Generates important Q&A pairs from the loaded document.
 * These are more like general discussion questions.
 * @returns {Promise<Array<{question: string, answer: string}>>} An array of Q&A objects.
 */
const generateImportantQA = async () => {
  console.log('Generating important Q&A pairs for general understanding...');
  const allChunks = documentModel.getAllDocumentChunks(); // Gets chunks of currently loaded document

  if (allChunks.length === 0) {
    throw new Error('No document chunks available for Q&A generation. Please upload or load a document first.');
  }

  const fullDocumentText = allChunks.map(doc => doc.pageContent).join('\n\n');

  const qaPrompt = PromptTemplate.fromTemplate(
    `Based on the following document, generate 3-5 important question-answer pairs that would be useful for a general understanding or discussion of this document. Format each pair as "Q: [Question]\nA: [Answer]".
    
    Document:
    {document}
    
    Q&A Pairs:`
  );

  const result = await model.invoke(await qaPrompt.format({ document: fullDocumentText }));

  const qaPairs = [];
  const lines = result.content.split('\n');
  let currentQuestion = '';
  for (const line of lines) {
    if (line.startsWith('Q:')) {
      currentQuestion = line.substring(2).trim();
    } else if (line.startsWith('A:') && currentQuestion) {
      qaPairs.push({
        question: currentQuestion,
        answer: line.substring(2).trim(),
      });
      currentQuestion = '';
    }
  }
  return qaPairs;
};

/**
 * Generates flashcard-style Q&A pairs from the loaded document.
 * These are concise and suitable for memorization.
 * @returns {Promise<Array<{question: string, answer: string}>>} An array of flashcard objects.
 */
const generateFlashcards = async () => {
  console.log('Generating flashcard Q&A pairs...');
  const allChunks = documentModel.getAllDocumentChunks(); // Gets chunks of currently loaded document

  if (allChunks.length === 0) {
    throw new Error('No document chunks available for flashcard generation. Please upload or load a document first.');
  }

  const fullDocumentText = allChunks.map(doc => doc.pageContent).join('\n\n');

  const flashcardPrompt = PromptTemplate.fromTemplate(
    `From the following document, generate 5-10 concise question-answer pairs that are ideal for flashcards. Focus on key facts, definitions, and important details.
    Format each pair strictly as a JSON array of objects, where each object has "question" and "answer" properties.
    Example: [{{ "question": "What is X?", "answer": "Y." }}, {{ "question": "Define Z.", "answer": "Z is..." }}]
    
    Document:
    {document}
    
    Flashcards (JSON array):`
  );

  const payload = {
    contents: [{
      role: "user",
      parts: [{ text: await flashcardPrompt.format({ document: fullDocumentText }) }]
    }],
    generationConfig: {
      responseMimeType: "application/json",
      responseSchema: {
        type: "ARRAY",
        items: {
          type: "OBJECT",
          properties: {
            "question": { "type": "STRING" },
            "answer": { "type": "STRING" }
          },
          "propertyOrdering": ["question", "answer"]
        }
      }
    }
  };

  const apiKey = process.env.GOOGLE_API_KEY;
  const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`;

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(payload)
  });

  const result = await response.json();

  if (result.candidates && result.candidates.length > 0 &&
      result.candidates[0].content && result.candidates[0].content.parts &&
      result.candidates[0].content.parts.length > 0) {
    const jsonString = result.candidates[0].content.parts[0].text;
    try {
      const parsedFlashcards = JSON.parse(jsonString);
      if (Array.isArray(parsedFlashcards) && parsedFlashcards.every(item => typeof item.question === 'string' && typeof item.answer === 'string')) {
        return parsedFlashcards;
      } else {
        console.warn("LLM generated unexpected flashcard format:", jsonString);
        return [];
      }
    } catch (parseError) {
      console.error("Failed to parse flashcards JSON:", parseError);
      console.error("Raw LLM response:", jsonString);
      return [];
    }
  } else {
    console.warn("LLM did not return valid flashcard content.");
    return [];
  }
};


module.exports = {
  processPdfDocument,
  loadDocumentContext, // Export the new function
  getAnswerFromDocument,
  getDocumentSummary,
  getKeyConcepts,
  generateImportantQA,
  generateFlashcards,
};
